# RNN에서 Transformer까지: 기억, 주의, 그리고 문맥의 철학

---

## 1. CNN이 멈춘 지점 ― ‘기억의 부재’

CNN은 **공간적 패턴**을 인식하는 데 탁월했지만,

**시간의 흐름**이나 **순서의 의미**는 담을 수 없었다.

> “CNN은 눈은 있지만 기억이 없습니다.”
> 
> 
> 교수님은 이렇게 표현하며,
> 
> 이미지의 공간 관계를 파악할 수는 있지만
> 
> “이전 순간의 정보가 다음 순간에 영향을 미치는 구조”는 CNN에는 없다고 강조했다.
> 

시계열 데이터나 문장처럼 **앞뒤 맥락이 중요한 데이터**를 이해하기 위해

새로운 구조가 필요했다.

그것이 바로 **RNN(Recurrent Neural Network)** 이다.

---

## 2. RNN ― 순서를 기억하게 하다

RNN은 “현재의 판단은 과거의 경험 위에 세워진다”는 철학을 모델로 구현했다.

즉, **이전 상태의 정보(기억)** 를 **현재의 계산**에 반영한다.

$h_t = f(W_xx_t + W_hh_{t-1} + b)$ 

이 식은 RNN이 **현재 입력(x_t)** 과 **이전 상태(h_{t-1})** 를 함께 사용하여

새로운 상태 $h_t$를 만든다는 뜻이다.

> “이전 단계의 출력이 다음 단계의 입력으로 흘러들어갑니다.
> 
> 
> 즉, 네트워크가 ‘시간의 축’을 따라 스스로 연결되어 있는 셈이에요.”
> 

RNN은 자연어, 음성, 시계열 등

**“순서가 의미를 가지는 데이터”**에 탁월했다.

### [수업 포인트]

- “RNN은 기억을 가진 첫 신경망이다.”
- “하지만 기억력이 짧습니다.”
    
    → 긴 문장을 처리할수록 **기울기 소실(Vanishing Gradient)** 이 발생한다.
    
- “결국 너무 많은 걸 다 기억하려다, 아무것도 못 기억하는 구조가 되어버렸어요.”

---

## 3. LSTM ― 망각의 공학화

RNN이 모든 과거를 붙잡으려 했던 반면,

**LSTM(Long Short-Term Memory)** 은 **‘잊을 줄 아는 기억’**을 설계했다.

> “사람도 모든 걸 기억하지 않아요.
> 
> 
> 중요한 것만 남기고, 나머지는 과감히 잊습니다.
> 
> LSTM은 바로 그 원리를 수학적으로 모델링한 겁니다.”
> 

LSTM은 내부에 세 가지 게이트를 둔다.

```latex
1. 입력 게이트 (Input Gate): 새 정보를 얼마나 받아들일지 결정
2. 망각 게이트 (Forget Gate): 이전 정보를 얼마나 버릴지 결정
3. 출력 게이트 (Output Gate): 어떤 정보를 외부로 내보낼지 결정
```

수식으로 표현하면 다음과 같다.

$f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)
i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i)
\tilde{C}_t = \tanh(W_c \cdot [h_{t-1}, x_t] + b_c)
C_t = f_t * C_{t-1} + i_t * \tilde{C}_t
o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o)
h_t = o_t * \tanh(C_t)$

> “망각 게이트(Forget Gate)는 신경망에 들어온 불필요한 기억을 청소하는 역할을 합니다.
> 
> 
> 인간의 ‘기억의 압축 알고리즘’과 비슷하죠.”
> 

### [수업 포인트]

- “LSTM은 **기억의 선택적 유지**를 통해 안정적인 장기 학습을 가능하게 했다.”
- “단순히 구조가 복잡해진 게 아니라, **기억의 철학이 들어간 모델**입니다.”
- “하지만 완벽하지 않아요. 여전히 먼 과거는 희석됩니다.”

---

## 4. GRU ― 단순하지만 강한 기억

교수님은 LSTM의 복잡성을 지적하며 GRU를 소개했다.

> “LSTM은 너무 똑똑하지만, 계산이 많아요.
> 
> 
> GRU는 ‘필요한 기억만 남기자’는 철학에서 나왔습니다.”
> 

GRU(Gated Recurrent Unit)는

입력 게이트와 망각 게이트를 하나의 **Update Gate**로 통합했다.

$z_t = \sigma(W_z \cdot [h_{t-1}, x_t])
r_t = \sigma(W_r \cdot [h_{t-1}, x_t])
\tilde{h}_t = \tanh(W \cdot [r_t * h_{t-1}, x_t])
h_t = (1 - z_t) * h_{t-1} + z_t * \tilde{h}_t$

> “GRU는 ‘기억과 갱신’의 균형만 남긴 단순화된 LSTM이에요.”
> 

[수업 포인트]

- 연산량이 적고 학습이 빠르다.
- LSTM보다 단순하지만, 성능은 거의 비슷하다.
- “LSTM이 인간이라면 GRU는 미니멀리스트 엔지니어예요.”

---

## 5. Attention ― 기억보다 주의

RNN·LSTM이 정보를 순차적으로 흘려보냈다면,

Attention은 이 흐름을 깨고 “**한눈에 전체를 본다**”는 개념을 도입했다.

> “Attention은 기억이 아니라 시선이에요.
> 
> 
> 필요한 곳에만 집중하고, 나머지는 무시합니다.”
> 

$Attention(Q, K, V) = Softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$ 

이 수식은 각 Query가 모든 Key와의 유사도를 계산하고,

그 유사도(Score)에 따라 Value를 가중합하는 과정이다.

> “RNN은 문장을 단어 순서대로 읽지만,
> 
> 
> Attention은 전체 문장을 한 번에 보고 ‘어떤 단어가 중요할지’를 결정합니다.”
> 

### [수업 포인트]

- 장거리 의존성을 자연스럽게 학습할 수 있다.
- 순차 계산이 아니라 병렬 계산이 가능하다.
- “Attention은 ‘의미의 연결’을 모델링한다.
    
    문장 안의 단어들이 서로에게 보내는 신호를 학습하는 거예요.”
    

---

## 6. Transformer ― 순서를 재정의한 구조

Transformer는 Attention의 철학을 확장해

**순서를 직접 학습하지 않고도 문맥을 이해**할 수 있는 모델을 만들었다.

> “Transformer는 ‘순서를 외우는’ 대신 ‘위치를 느끼는’ 모델이에요.”
> 

입력에 **Positional Encoding**을 더해 순서를 알려준다.

$PE(pos, 2i) = sin\left(\frac{pos}{10000^{2i/d}}\right)
PE(pos, 2i+1) = cos\left(\frac{pos}{10000^{2i/d}}\right)$

> “사람은 말의 순서를 듣지만, Transformer는 각 단어에 위치를 태그로 붙여서 학습합니다.
> 
> 
> 순서는 암기하는 게 아니라 ‘느끼는 것’이죠.”
> 

### [수업 포인트]

- 순서를 벡터로 표현하여 병렬 학습 가능.
- Attention 기반이므로 장거리 문맥 처리 우수.
- “Transformer는 언어를 ‘의미의 공간’으로 재배열한 모델이다.”

---

## 7. Vision Transformer (ViT) ― 이미지를 문장처럼 읽다

> “ViT는 이미지에 Transformer를 얹은 모델이 아니라,
> 
> 
> 이미지를 ‘언어처럼 이해하려는 시도’입니다.”
> 

ViT는 이미지를 **패치(patch)** 단위로 쪼개고,

각 패치를 하나의 단어(token)처럼 취급한다.

$이미지 → 패치 분할 → 패치 임베딩 + 위치 인코딩 → Transformer Encoder$ 

> “CNN은 점진적으로 넓은 시야를 형성하지만,
> 
> 
> ViT는 처음부터 전체를 봅니다.
> 
> 그래서 깊이보다는 데이터량이 중요해졌죠.”
> 

[수업 포인트]

- CNN은 구조적 규칙(Inductive Bias)에 의존하지만,
    
    ViT는 **데이터 자체로부터 규칙을 학습한다.**
    
- “CNN은 ‘규칙의 모델’, ViT는 ‘데이터의 모델’이다.”
- 데이터가 많을수록 ViT는 압도적으로 강력하다.

---

## 8. DEiT ― ViT에게 가르침을 주다

데이터가 적으면 ViT는 불안정하다.

이 한계를 해결하기 위해 **DEiT(Data-efficient Image Transformer)** 가 등장했다.

> “DEiT는 선생님(CNN)에게서 배우는 Transformer입니다.
> 
> 
> 단순히 정답을 외우는 게 아니라,
> 
> **‘선생님이 왜 그렇게 판단했는가’를 배우는 학생 모델**이에요.”
> 

### [수업 포인트]

- Knowledge Distillation으로 ViT 학습 효율 향상.
- 적은 데이터에서도 Transformer 성능 확보.
- “DEiT는 CNN과 Transformer의 화해입니다.
    
    규칙과 데이터가 손을 잡은 모델이에요.”
    

---

## 교수님의 핵심 정리

> “RNN은 ‘기억’을 설계했습니다.
> 
> 
> LSTM은 ‘망각’을 설계했습니다.
> 
> Attention은 ‘집중’을 설계했습니다.
> 
> Transformer는 ‘순서를 재정의’했습니다.
> 
> ViT는 ‘언어의 철학을 시각으로 확장’했습니다.”
> 

> “이 모든 발전의 공통점은,
> 
> 
> **‘인간의 사고 과정을 점점 더 닮아가는 수학적 구조화’**라는 데 있습니다.”
>