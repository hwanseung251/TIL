# 사전학습(Pre-training)과 미세조정(Fine-tuning), BERT의 구조

## 1. 언어모델의 진화 방향

언어모델은 **단어의 확률 예측 → 문맥 이해 → 문장 의미 표현**으로 발전해왔다.

Transformer의 등장 이후, 모델은 단순한 예측을 넘어 **사전학습(Pre-training)**을 통해

일반적인 언어 능력을 습득하고, **미세조정(Fine-tuning)**으로 특정 작업에 적응하는 구조로 진화했다.

---

## 2. 사전학습(Pre-training)의 개념

사전학습은 대규모 말뭉치(Corpus)에서 언어의 패턴을 먼저 학습하는 과정이다.

대표적인 학습 목표는 두 가지이다.

### 1️⃣ Masked Language Modeling (MLM)

입력 문장의 일부 단어를 [MASK]로 가리고, 그 단어를 예측한다.

```latex
입력: "나는 오늘 [MASK]을 먹었다."
출력: "나는 오늘 라면을 먹었다."
```

이 방식은 문맥을 기반으로 단어를 복원해야 하므로,

모델이 **양방향(Bidirectional)** 문맥을 이해하도록 유도한다.

---

### 2️⃣ Next Sentence Prediction (NSP)

두 문장이 주어졌을 때, 두 번째 문장이 첫 번째 문장의 연속인지 판별한다.

```latex
문장 A: "나는 밥을 먹었다."
문장 B: "이어서 커피를 마셨다." → IsNext
문장 B: "고양이가 창밖을 봤다." → NotNext
```

이 과정을 통해 모델은 **문장 간 관계(Context-level coherence)**를 학습한다.

---

## 3. Fine-tuning (미세조정)

사전학습으로 얻은 일반 언어모델에 **작업(Task) 특화 데이터를 추가 학습**하는 단계이다.

이 과정을 통해 모델은 감정분석, 문장분류, 질의응답 등 다양한 NLP 과제에 적응한다.

```latex
Fine-tuning = Pre-trained Model + Task-specific Head
```

[추가설명]

BERT는 Fine-tuning 시 **가중치 전체를 업데이트**하지만,

GPT-2 이후 등장한 LoRA 등은 일부 파라미터만 조정한다(효율적 미세조정).

---

## 4. BERT의 구조 개요

BERT(Bidirectional Encoder Representations from Transformers)는

Transformer의 **Encoder 구조**만을 사용하는 양방향 언어모델이다.

### 핵심 아이디어

- RNN 기반 모델은 좌→우, 혹은 우→좌 방향만 보았지만,
    
    BERT는 **좌·우 문맥을 동시에 학습**한다.
    
- 입력 문장은 `[CLS]` 토큰으로 시작하며, `[SEP]`으로 구분된다.

```latex
입력: [CLS] 나는 오늘 [MASK]을 먹었다 [SEP]
```

[CLS]의 최종 벡터는 **문장 전체의 의미 표현(embedding)**으로 사용된다.

---

## 5. BERT 입력 구성요소

BERT의 입력은 세 가지 벡터의 합으로 구성된다.

```latex
Input Embedding = Token Embedding + Segment Embedding + Position Embedding
```

- **Token Embedding:** 단어의 의미
- **Segment Embedding:** 문장이 A인지 B인지 구분
- **Position Embedding:** 단어의 위치 정보

---

## 6. BERT의 Encoder 구조

Transformer Encoder를 N층 반복하며, 각 층은 다음 두 모듈로 구성된다.

```latex
1. Multi-Head Self-Attention
2. Feed Forward Network
```

Self-Attention을 통해 문맥을 종합하고, FFN으로 비선형 변환을 수행한다.

각 층에는 **Residual Connection**과 **Layer Normalization**이 적용된다.

---

## 7. Self-Attention 복습

Self-Attention은 입력 단어들의 관계를 반영하는 메커니즘이다.

$Attention(Q, K, V) = Softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

여기서 $Q,K,V$는 각각 Query, Key, Value 행렬이며,

내적값이 커질수록 Softmax의 출력이 뾰족해지므로 $\sqrt{d_k}$로 나누어 정규화한다.

---

## 8. BERT의 사전학습 목표

BERT는 두 가지 학습 목표를 동시에 수행한다.

$L = L_{MLM} + L_{NSP}$

- $L_{MLM}$: Masked Language Modeling 손실
- $L_{NSP}$: Next Sentence Prediction 손실

이 두 목표를 함께 최적화 함으로써, 단어 수준과 문장 수준의 이해력을 동시에 얻는다.

---

## 9. BERT의 양방향성(Bidirectionality)

기존 언어모델(GPT, RNN 등)은 한 방향의 문맥만 본다.

반면, BERT는 Transformer의 **Encoder의 Self-Attention**을 사용해

모든 단어가 서로를 참고할 수 있다.

$Attention(Q, K, V) = Softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$ 

이 구조 덕분에 BERT는 문장의 앞뒤 관계를 모두 고려한 언어 표현을 학습한다.

---

## 10. BERT Fine-tuning의 다양한 응용

| 작업(Task) | 입력 형태 | 출력 | 예시 |
| --- | --- | --- | --- |
| 문장 분류 | [CLS] 문장 [SEP] | [CLS] 벡터 → Softmax | 감정 분석 |
| 문장쌍 분류 | [CLS] 문장1 [SEP] 문장2 [SEP] | [CLS] → Softmax | 문장 관계 판별 |
| 토큰 분류 | [CLS] + 문장 내 단어들 | 각 단어별 라벨 | 개체명 인식(NER) |
| 질의응답(QA) | 질문 [SEP] 문서 | 시작·끝 토큰 확률 | SQuAD 등 |

---

## 11. BERT 학습 파이프라인

```python
# Pre-training
for batch in corpus:
    masked_tokens = mask_random_words(batch)
    outputs = model(masked_tokens)
    loss = MLM_loss(outputs) + NSP_loss(outputs)
    loss.backward()
    optimizer.step()

# Fine-tuning
for batch in task_dataset:
    outputs = model(batch)
    task_loss = compute_loss(outputs, labels)
    task_loss.backward()
    optimizer.step()

```

[추가설명]

Fine-tuning 시 전체 파라미터를 미세 조정함으로써,

다양한 NLP 태스크에 맞는 표현을 학습하게 된다.

---

## 12. BERT의 한계와 확장

| 한계 | 해결 모델 |
| --- | --- |
| Next Sentence Prediction의 비효율 | RoBERTa: NSP 제거, 더 많은 데이터로 학습 |
| 긴 문장 처리의 한계 | Longformer, BigBird |
| 사전학습 비용 높음 | DistilBERT (경량화) |
| 단일 언어 중심 | mBERT, XLM-R (다국어 지원) |

---

## 13. 사전학습 모델의 발전 계보

| 세대 | 모델 | 특징 |
| --- | --- | --- |
| **1세대** | Word2Vec, GloVe | 단어 수준 의미 표현 |
| **2세대** | ELMo | 양방향 LSTM 기반 문맥 임베딩 |
| **3세대** | BERT | Transformer 기반 양방향 문맥 이해 |
| **4세대** | GPT, T5 | 생성 중심, 사전학습+미세조정 통합 |
| **5세대** | LLaMA, Gemma, GPT-4 | 초거대 모델, 멀티태스크/멀티모달 |

---

## 14. 핵심 요약

1. **Pre-training**은 일반 언어 지식을 대규모로 학습하는 단계이다.
2. **Fine-tuning**은 사전학습 모델을 특정 작업에 맞게 조정하는 과정이다.
3. **BERT**는 양방향 Transformer Encoder 구조를 사용하여 문맥 이해력을 극대화했다.
4. **MLM + NSP**의 결합으로 단어와 문장 관계를 동시에 학습한다.
5. 이후 발전 모델(RoBERTa, T5 등)은 효율성과 확장성을 높여 LLM 시대로 진입했다.