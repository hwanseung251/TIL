## 1. 강의 학습 목표

1. **비전(이미지) 기반 AI 모델의 발전 흐름 이해**
    - CNN에서 시작하여 Vision Transformer(ViT)까지의 발전 과정 학습
2. **멀티모달 AI 개념 파악**
    - 언어, 이미지, 소리 등 다양한 모달리티 간의 상호 작용 이해
3. **파운데이션 모델의 비전 확장 구조 이해**
    - 이미지 중심 대규모 사전학습 모델의 구조 및 응용
4. **실제 산업 응용 사례 탐색**
    - 비전 모델이 자율주행, 로보틱스, 생성 AI 등에서 어떻게 쓰이는지 이해

---

## 2. 비전 AI의 흐름 요약

> “언어 모델이 GPT로 이어졌듯,
> 
> 
> 이미지 모델은 CNN에서 Transformer로 넘어가며 **Foundation화**되고 있습니다.”
> 

### 2.1 CNN의 시대

- **AlexNet (2012)**: GPU 기반으로 딥러닝 부흥의 시초
- **VGGNet**: 구조의 규칙성과 단순함 강조 (3×3 Conv 반복)
- **ResNet**: 깊은 네트워크의 기울기 소실을 **Skip Connection**으로 해결
- **EfficientNet**: **규모 확장(Scaling)** 을 수학적으로 정립 (너비·깊이·해상도 동시 제어)

> “CNN은 공간 패턴(공간적 국소성)을 잘 잡지만,
> 
> 
> **전역적 문맥(Global Context)** 은 보지 못합니다.”
> 

---

### 2.2 Vision Transformer (ViT)의 등장

> “Transformer가 NLP에서 성공했을 때,
> 
> 
> ‘그럼 이미지는 왜 안 되지?’ 하는 질문에서 출발했습니다.”
> 

### 핵심 아이디어

- 이미지를 **Patch 단위(예: 16×16)** 로 잘라서,
    
    각 Patch를 토큰(token)처럼 취급 → Transformer 입력으로 사용.
    
- Attention으로 **전역적 문맥**을 파악 → CNN의 지역성 한계를 극복.

$\text{Image} \rightarrow \text{Patch Embedding} \rightarrow \text{Transformer Encoder} \rightarrow \text{Classification Head}$

### ViT의 주요 특징

- 구조는 언어 Transformer와 동일.
- CNN보다 **더 많은 데이터와 연산량**을 요구.
- 작은 데이터셋에서는 오히려 CNN보다 성능이 떨어짐 → **사전학습(Pretraining)** 필요.

> “ViT는 데이터가 많을수록 좋아지는 모델입니다.
> 
> 
> 작은 데이터에서는 CNN이 여전히 효율적이에요.”
> 

---

## 3. CNN vs Transformer 비교 요약

| 구분 | CNN | Vision Transformer |
| --- | --- | --- |
| 구조 특징 | 지역적 패턴 감지 | 전역적 문맥 학습 |
| 학습 효율 | 적은 데이터에도 잘 학습 | 대규모 데이터 필요 |
| 사전학습 | 상대적으로 단순 | 필수적 (거대 데이터 필요) |
| 연산량 | 비교적 가볍다 | 매우 무겁다 |
| 장점 | 빠르고 안정적 | 강력한 표현력, 범용성 |
| 활용 예시 | 모바일·엣지 | 클라우드·대형 모델 |

---

## 4. 멀티모달 AI의 등장

> “Foundation Model의 다음 진화는
> 
> 
> 단일 모달에서 **멀티모달(Multimodal)** 로 가는 겁니다.”
> 

### 개념 정리

- **멀티모달**: 서로 다른 유형의 데이터를 통합적으로 처리하는 모델
    
    (예: 텍스트 + 이미지, 음성 + 영상 등)
    
- GPT-4V, Gemini, Kosmos, CLIP 등
    
    → 모두 언어와 이미지를 함께 이해하는 모델
    

### 구조 개념

- **Encoder-Decoder Framework**
    - Encoder: 입력(이미지, 텍스트 등)을 임베딩
    - Decoder: 다른 형태의 출력 생성 (예: 이미지 캡션, 질문 응답 등)

$\text{Input (Image + Text)} \rightarrow \text{Encoder} \rightarrow \text{Decoder} \rightarrow \text{Output (Text/Label)}$ 

### CLIP (Contrastive Language-Image Pretraining)

- OpenAI의 대표적 멀티모달 모델
- **텍스트-이미지 쌍**을 대규모로 학습하여
    
    텍스트와 이미지의 의미 공간을 **같은 임베딩 공간**으로 투영.
    

> “CLIP은 ‘이미지 분류기’가 아니라
> 
> 
> **이미지 의미를 언어적으로 이해하는 모델**입니다.”
> 

---

## 5. 이미지 생성과 Diffusion Model

> “GAN 이후의 대세는 Diffusion입니다.
> 
> 
> 노이즈를 점진적으로 제거하면서 이미지를 만들어내죠.”
> 

### 기본 원리

1. **순방향 과정(Forward)**
    - 깨끗한 이미지를 점점 노이즈로 변환
2. **역방향 과정(Reverse)**
    - 노이즈로부터 이미지를 복원
    - 모델이 **노이즈를 제거하는 함수**를 학습함.
    
    $x_t = \sqrt{\alpha_t}x_{t-1} + \sqrt{1 - \alpha_t}\epsilon, \quad \epsilon \sim \mathcal{N}(0, 1)$
    

### 특징

- 안정적인 학습 (GAN보다 발산 적음)
- 매우 고해상도, 사실적 이미지 생성
- 텍스트 조건 입력 → **Text-to-Image (예: Stable Diffusion)**

---

## 6. 멀티모달 시대의 비전 모델 방향

| 핵심 키워드 | 설명 |
| --- | --- |
| **융합 (Integration)** | 언어, 이미지, 소리 등 다중 정보 동시 처리 |
| **확장 (Scalability)** | 거대 데이터 + 클라우드 인프라 기반 |
| **적응 (Adaptation)** | 프롬프트/LoRA 기반 소량 튜닝 |
| **해석 (Interpretability)** | 모델의 판단 근거 시각화 및 검증 |
| **생성 (Generation)** | Text-to-Image, Text-to-Video 등 확장 |

> “결국 멀티모달은
> 
> 
> **‘AI가 세상을 사람처럼 이해하는 단계’로 가는 과정**입니다.”
> 

---

## 7. 실무 활용 포인트

1. **사전학습된 모델 적극 활용**
    - CLIP, BLIP, DINO, SAM 등 이미 강력한 베이스 존재
2. **작은 데이터 → Fine-tuning or LoRA 기반 적응**
3. **GPU 효율화**
    - Mixed Precision, Gradient Checkpointing 활용
4. **도메인 적응**
    - 예: 금융문서 → OCR + Text Embedding
    - 제조 → Vision Embedding + Sensor Data 융합

---

## 8. 교수님 핵심 멘트

> “이제 AI는 텍스트만이 아니라,
> 
> 
> **세상의 모든 신호를 한꺼번에 이해하는 단계**로 갑니다.”
> 
> “멀티모달의 본질은 모델이 아니라,
> 
> **‘세상 인식의 통합’**이에요.”
> 
> “이제 여러분은 모델을 만드는 게 아니라,
> 
> **모델을 활용해 나만의 문제를 해결하는 법**을 배워야 합니다.”
>