# Self-Attention과 Transformer의 구조와 원리

## 1. RNN의 한계와 Self-Attention의 필요성

### RNN의 구조적 제약

RNN은 입력 시퀀스를 순차적으로 처리하며, 이전 상태 $h_{t-1}$을 이용해 다음 상태 $h_t$ 를 계산한다.

이로 인해 **시간 순서 의존성**이 발생하여 병렬 처리가 불가능하고, 긴 문장에서 **장기 의존성(Vanishing Gradient)** 문제가 나타난다.

[추가설명]

문장이 길어질수록 초반 정보가 사라지고, 모델이 뒷부분 단어를 해석할 때 앞의 맥락을 잃는 현상이 발생한다.

---

## 2. Self-Attention의 개념

Self-Attention은 하나의 문장(시퀀스) 내부에서 단어 간 상호 관계를 학습하는 메커니즘이다.

각 단어는 다른 모든 단어를 참고하여, 문맥을 반영한 새로운 표현 벡터를 만든다.

[추가설명]

RNN이 단어를 하나씩 읽는 ‘낭독형’이라면, Self-Attention은 전체 문장을 한눈에 바라보는 ‘속독형’ 구조이다.

---

## 3. Query, Key, Value의 정의

각 단어의 임베딩 벡터 $x_i$로부터 세 개의 벡터를 생성한다.

- $Q_i = W_Q x_i$
- $K_i = W_K x_i$
- $V_i = W_V x_i$

여기서 $W_Q, W_K, W_V$는 **학습 가능한 파라미터 행렬**이며,

**모든 단어가 동일한** $W_Q, W_K, W_V$를 공유한다.

[추가설명]

즉, 단어마다 다른 행렬을 가지는 것이 아니라, 같은 가중치 행렬을 사용하여 공통적인 의미 공간으로 투영한다.

---

## 4. Attention Score 계산 과정

### 1️⃣ Query-Key 유사도 계산

각 단어 i의 Query와 모든 단어 j의 Key 벡터를 내적하여 유사도를 계산한다.

$score(i, j) = Q_i · K_j$

### 2️⃣ Softmax를 이용한 확률화

유사도를 확률로 정규화한다.

$α_{ij} = \frac{exp(score(i, j))}{∑_k exp(score(i, k))}$

### 3️⃣ Value의 가중합으로 새로운 표현 생성

Value 벡터들을 확률 가중치로 합산한다.

$h_i = ∑_j α_{ij} V_j$

[추가설명]

이로써 $h_i$는 단어 $i$ 자신의 의미뿐 아니라 문장 전체의 문맥 정보를 포함한다.

---

## 5. Scaled Dot-Product Attention

차원이 커질수록 내적 값이 커져 Softmax 출력이 지나치게 뾰족해지는 문제를 방지하기 위해,

유사도 값을 $\sqrt{d_k}$로 나눈다.

$Attention(Q, K, V) = Softmax\left(\frac{QK^T}{\sqrt{d_k}}\right)V$

[추가설명]

예를 들어 $d_k = 64$이면 $\sqrt{d_k} = 8$로, 분모를 통해 안정적인 확률 분포를 만든다.

---

## 6. Self-Attention의 병렬성

Self-Attention은 각 단어의 $Q, K, V$가 독립적으로 계산되므로,

모든 단어의 연산을 동시에 수행할 수 있다.

→ **병렬 연산 가능**, 학습 속도 대폭 향상.

RNN은 순차적이라 GPU 병렬화가 어렵지만, Self-Attention은 **GPU에 최적화된 구조**다.

---

## 7. Multi-Head Attention

Self-Attention을 여러 개 병렬로 수행하는 구조이다.

서로 다른 파라미터 집합 $W_Q^i, W_K^i, W_V^i$를 사용하여 여러 방향의 문맥을 동시에 학습한다.

각 Head의 계산식은 다음과 같다.

$head_i = Attention(QW_Q^i, KW_K^i, VW_V^i)$

모든 Head를 연결(concatenate)하여 하나의 벡터로 만든다.

$MultiHead(Q, K, V) = Concat(head_1, ..., head_h) W_O$

[추가설명]

각 Head는 문장의 서로 다른 관계(시제, 감정, 의미, 문법 구조 등)에 주목한다.

다양한 관점의 정보를 통합함으로써 문맥 이해력이 향상된다.

---

## 8. Positional Encoding

Self-Attention은 순서를 고려하지 않기 때문에, 단어의 **위치 정보**를 따로 주입해야 한다.

이를 위해 각 단어의 임베딩에 위치 벡터를 더한다.

$Input_i = Embedding(w_i) + PositionalEncoding(i)$

Positional Encoding은 다음과 같이 정의된다.

$PE(pos, 2i) = \sin\left(\frac{pos}{10000^{2i/d}}\right)$

$PE(pos, 2i+1) = \cos\left(\frac{pos}{10000^{2i/d}}\right)$

[추가설명]

사인/코사인 함수를 이용하면 위치가 연속적으로 표현되어, 모델이 인접 단어 간의 거리 차이를 인식할 수 있다.

---

## 9. Feed-Forward Layer

Self-Attention은 선형 연산의 조합이므로 비선형성이 부족하다.

이를 보완하기 위해 각 Attention 블록 뒤에는 **ReLU 활성화 함수를 가진 Feed-Forward Network(FFN)**이 존재한다.

- 비선형성을 추가하여 복잡한 함수 표현 가능
- 학습 안정성과 일반화 성능 향상

[추가설명]

RNN은 깊이가 제한되지만, Transformer는 수십 층의 Feed-Forward 블록을 안정적으로 쌓을 수 있다.

---

## 10. Transformer의 전체 구조

Transformer는 인코더와 디코더로 구성된다.

- **Encoder**
    
    입력 문장을 받아 문맥을 벡터로 인코딩한다.
    
    각 인코더 블록 = Self-Attention + Feed-Forward
    
- **Decoder**
    
    이전에 생성된 단어를 바탕으로 다음 단어를 예측한다.
    
    구성 요소 = Self-Attention + Encoder-Decoder Attention + Feed-Forward
    

[추가설명]

디코더의 Self-Attention에는 **마스크(Masking)**가 적용되어,

미래 단어의 정보를 참고하지 못하게 한다.

이는 Softmax 입력에서 미래 단어의 가중치를 $−∞$로 설정함으로써 구현된다.

---

## 11. ReLU와 tanh의 비교

| 항목 | ReLU | tanh |
| --- | --- | --- |
| **출력 범위** | $[0,∞)$ | $[−1,1]$ |
| **특징** | 음수는 0으로 제거 → 희소 표현 | 작은 값 범위 → 정보 압축 |
| **적용 예시** | Transformer, CNN | RNN, LSTM |
| **효과** | 빠른 학습, 깊은 층에서 안정적 | 순차 구조의 부드러운 변화에 적합 |

[추가설명]

ReLU는 불필요한 음수값을 제거하여 **효율적이고 깊은 학습**을 가능하게 한다.

---

## 12. Multi-Layer Transformer

- 각 인코더/디코더 블록은 동일 구조이지만 **파라미터를 공유하지 않는다.**
- 블록이 많아질수록 표현력이 커지지만, 과적합 위험도 증가한다.
- 실험적으로 6~12층 구성이 가장 안정적인 성능을 보인다.

---

## 핵심 요약

1. Self-Attention은 문장 내부 단어 간 관계를 병렬로 학습한다.
2. Query-Key-Value 구조로 문맥 가중치를 계산한다.
3. Positional Encoding으로 순서 정보를 보완한다.
4. Feed-Forward와 ReLU로 비선형성 강화.
5. Transformer는 이 구조를 반복 쌓아 딥러닝의 핵심 모델로 발전했다.