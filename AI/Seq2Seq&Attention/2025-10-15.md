# 언어 모델의 발전과 Seq2Seq, Attention의 등장

## 1. 언어 모델(Language Model)의 개념

### 언어 모델의 정의

- **언어 모델(Language Model, LM)**은 “문맥이 주어졌을 때 다음 단어가 등장할 확률”을 계산하는 모델이다.
- 예를 들어 “The cat sat on the mat.”라는 문장의 확률은 다음과 같은 곱으로 표현된다
    
    $P(w_1, w_2, \dots, w_n) = \prod_{i=1}^{n} P(w_i | w_1, \dots, w_{i-1})$
    

### 직관적 이해

- 인간의 언어는 **모든 단어의 랜덤 조합**이 아니라 **자연스러운 확률적 규칙**을 따른다.
    
    예:
    
    - “I love you” → 높은 확률
    - “You I love” → 낮은 확률
- 즉, 언어모델은 “자연스러운 문장의 확률 공간”을 학습하는 모델이다.

[추가설명]

스마트폰의 자동완성, 검색엔진의 추천 문구, GPT의 문장 생성 모두 “다음 단어의 확률 예측” 원리를 활용한다.

---

## 2. 전통적 통계 기반 언어 모델: N-gram

### N-gram의 개념

- **N개의 연속된 단어 묶음**을 기반으로 문맥을 추정하는 통계적 모델.
    - Unigram: 단어 하나씩 (예: “The”, “student”)
    - Bigram: 2개씩 (예: “The student”)
    - Trigram: 3개씩 (예: “The student open”)
    - 4-gram, 5-gram으로 확장 가능.

### 동작 원리

- 대규모 말뭉치에서 **N개의 단어 조합이 나타난 빈도수**를 세어 확률을 계산한다.
    - 예:
        
        “The student open day” → 다음 단어 “book”이 1000회 중 400회 등장
        
        ⇒ $P(\text{book} | \text{The student open day}) = \frac{400}{1000} = 0.4$
        

### 장점

- 간단하고 직관적이다.
- 특정 문맥에서 다음 단어 확률을 **빈도 기반으로 바로 계산 가능**하다.

### 한계

1. **문맥의 길이 제약**
    - N이 커질수록 가능한 조합의 수가 폭발적으로 증가하여 학습 데이터가 희소해진다.
    - “시험 감독관이 시계를 켰다” 문맥처럼 길이가 긴 경우, 4-gram으로는 올바른 문맥을 포착하지 못한다.
2. **의미적 일반화 불가**
    - “학생이 책을 읽었다”와 “아이가 책을 읽었다”는 의미상 유사하지만, N-gram은 이를 구별하지 못한다.

[추가설명]

이 시기에는 **임베딩(embedding)** 개념이 없었기 때문에, 단어를 단순한 문자열로만 취급했다.

---

## 3. 통계적 기계 번역(Statistical Machine Translation)

### 개념

- N-gram 언어모델과 확률 기반 번역모델을 결합한 초기 번역 방식.
- **입력(X)**: 한국어 문장
    
    **출력(Y)**: 영어 문장
    
    → “X를 주었을 때 Y의 확률이 높은 조합”을 찾는다.
    
- 수학적으로는 다음을 최대화하는 방식이다
    
    $\hat{Y} = \arg\max_Y P(Y | X) = \arg\max_Y P(X | Y)P(Y)$
    

### 한계

- 실제 번역은 수많은 규칙 기반, 빈도 기반 모듈이 결합된 거대한 파이프라인 구조였다.
- 사람 수십 명이 시스템을 튜닝해야 했고, 문법적으로 어색한 번역이 자주 발생했다.

---

## 4. 딥러닝 기반의 전환: Seq2Seq (Sequence to Sequence)

### 등장 배경

- 기존 N-gram 및 통계 기반 번역은 **긴 문맥 처리**가 어렵고, 시스템이 복잡했다.
- 구글이 2014년 제안한 **Seq2Seq 구조**는 **RNN 기반 인코더-디코더 구조**로 번역을 단일 신경망으로 처리했다.

### 구조

1. **인코더 (Encoder):**
    
    입력 문장을 순차적으로 읽어 **하나의 벡터 표현(h)** 으로 압축한다.
    
2. **디코더 (Decoder):**
    
    인코더의 벡터를 기반으로 한 단어씩 출력 문장을 생성한다.
    
    → 오토리그레시브(Autoregressive): 이전에 생성한 단어를 다음 단계의 입력으로 사용.
    

[추가설명]

입력 문장의 시작과 끝은 `SOS`(Start of Sentence), `EOS`(End of Sentence) 같은 **스페셜 토큰**으로 구분한다.

### 예시

입력: “The black cat drank milk.”

출력: “Le chat noir a bu du lait.”

---

## 5. Seq2Seq의 학습 방식

### Teacher Forcing

- 학습 중에는 정답 문장을 이미 알고 있으므로, **이전 정답 단어를 입력으로 사용**하여 다음 단어를 예측한다.
- 즉, “학습 중에는 교사가 매 단계 정답을 알려주는 방식”.
- 장점: 학습 안정성 증가
- 단점: 실제 추론 시(정답이 없는 상황) 오차 누적이 발생할 수 있다.

---

## 6. 디코딩 방식의 발전: Greedy vs Beam Search

| 구분 | Greedy Search | Beam Search |
| --- | --- | --- |
| **원리** | 매 단계에서 확률이 가장 높은 단어만 선택 | 확률 상위 K개의 후보 문장을 동시에 탐색 |
| **장점** | 빠르지만, 국소 최적해에 머무를 수 있음 | 다양한 후보를 고려해 더 자연스러운 문장 생성 가능 |
| **비유** | 한 번에 한 문장만 생각하며 말함 | 여러 가능한 문장을 머릿속에서 동시에 고려 |

[추가설명]

Beam Search는 K개의 후보를 **레이저 빔처럼 일정하게 유지**하면서 탐색한다는 의미에서 붙여진 이름이다.

---

## 7. Seq2Seq의 한계: 정보 압축 문제 (Bottleneck)

- 긴 문장을 하나의 벡터에 압축할 경우, **앞부분 정보가 손실**되는 문제가 발생.
- 예: “700페이지 해리포터 책을 읽고 책을 덮은 후 번역하라”는 것과 같다.
- 인코더 마지막 히든 벡터 하나로 전체 의미를 저장하기는 불가능.

---

## 8. 해결책: Attention Mechanism

### 핵심 개념

- **“중요한 단어에 더 집중하자.”**
- 디코더가 매 단어를 생성할 때, 인코더의 전체 히든 벡터 중 **어떤 부분을 참고할지 가중치를 학습**한다.

### 동작 원리

1. 디코더의 현재 상태(검색어 역할)를 sts_tst라 하고,
2. 인코더의 각 단어 히든벡터와 유사도를 계산해 **스코어(score)**를 얻는다.
3. Softmax를 적용해 가중치를 계산한다.
    
    $\alpha_{t,i} = \frac{\exp(\text{score}(s_t, h_i))}{\sum_{j}\exp(\text{score}(s_t, h_j))}$
    
4. 이 가중치를 이용해 **컨텍스트 벡터(Context Vector)**를 구한다
    
    $c_t = \sum_i \alpha_{t,i} h_i$
    
5. 최종 출력은 (디코더 상태 + 문맥정보)로 예측한다.
    
    $y_t = f(s_t, c_t)$
    

[추가설명]

이 메커니즘 덕분에 디코더는 필요한 순간마다 “책을 다시 들춰보듯” 문맥을 참고할 수 있다.

---

## 9. Attention의 효과

| 비교 항목 | Seq2Seq (기존) | Seq2Seq + Attention |
| --- | --- | --- |
| **문맥 유지력** | 인코더 마지막 벡터 하나에 의존 | 인코더의 전체 문맥 중 관련 단어에 집중 |
| **장기 문맥 처리** | 어려움 (700페이지 기억 문제) | 개선 (필요한 부분만 선택적으로 기억) |
| **성능** | 문장이 길어질수록 급격히 하락 | 긴 문장에서도 안정적으로 유지 |
| **학습 안정성** | 베니싱 그래디언트 문제 존재 | 병렬 연결 덕분에 완화 |
| **해석 가능성** | 내부 작동 불투명 | 어떤 단어에 주목했는지 시각화 가능 (Attention Map) |

[추가설명]

Attention은 단순한 성능 향상뿐 아니라 **모델 해석 가능성(Explainability)**을 높이는 계기가 되었다.

---

## 10. 발전의 흐름 요약

| 세대 | 대표 기술 | 핵심 아이디어 | 장점 | 한계 |
| --- | --- | --- | --- | --- |
| **1세대** | N-gram | 단어 빈도 기반 통계 | 단순하고 직관적 | 문맥 제한, 희소성 문제 |
| **2세대** | RNN / LSTM | 순차적 문맥 학습 | 시퀀스 처리 가능 | 장기 의존성 한계 |
| **3세대** | Seq2Seq | 인코더-디코더 구조로 번역 | End-to-End 학습 가능 | 정보 압축(Bottleneck) |
| **4세대** | Attention | 중요 단어 선택적 집중 | 성능 향상, 해석 가능성 증가 | 연산량 증가 |
| **5세대 이후** | Transformer | 완전 병렬 Attention 구조 | 효율, 확장성, 대규모 학습 가능 | (다음 강의 주제) |

---

## 핵심 요약

- 언어 모델은 “다음 단어 확률”을 예측하는 모델에서 시작해, 통계 기반 → 신경망 기반 → 어텐션 기반으로 발전했다.
- **Seq2Seq**는 “입력과 출력이 모두 시퀀스”인 문제를 딥러닝으로 직접 학습할 수 있게 한 혁신이었다.
- **Attention**은 “모델이 어디를 봐야 하는지”를 스스로 학습하여 성능뿐 아니라 해석력을 향상시켰다.
- 이후 등장한 **Transformer**는 Attention의 개념을 완전히 확장해 현재의 GPT, BERT 등 LLM의 기반이 되었다.