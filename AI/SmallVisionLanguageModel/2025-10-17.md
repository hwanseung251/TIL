## 1. 학습 목표

- **비전-언어 모델(VLM)**의 개념과 구조 이해
- 대규모 모델이 가진 **운영 비용 및 한계** 인식
- **경량화 모델(Small / On-device VLM)**의 필요성과 구현 전략 파악
- **한국어 VLM**이 별도로 필요한 이유 이해
- 실제 서비스나 프로젝트에서 Small VLM을 **효율적으로 배포 및 활용**하는 방법 습득

---

## 2. VLM의 등장 배경과 현실적 한계

### 2.1. 거대 모델의 비용 문제

파운데이션 모델(LLM, VLM)은 놀라운 성능을 보이지만,

교수님은 “**실행 자체가 쉽지 않다**”라고 강조하셨다.

GPU 자원이 없거나, 서버 구축이 어려운 환경에서는 단순 추론조차 불가능한 경우가 많다
251017 AI강의Ⅱ - SSAFY AI강의Ⅱ 이론3(….

- **서버 비용이 너무 높다.**
    - 추론 과정에서도 GPU 사용량이 상당하며, 유지비용이 크다.
- **데스크탑, 노트북, 심지어 모바일에서**도 동작 가능한 모델이 필요하다.

따라서 현실적으로 “**작지만 실용적인 모델**”의 가치가 커지고 있다.

---

## 3. Small / On-device Vision Language Model 개념

### 3.1. 정의

- **Small VLM (경량 비전-언어 모델)**
    
    : 성능은 다소 낮지만, 개인 PC·모바일·엣지 디바이스에서도 구동 가능한 모델.
    
    - 예: Small VLM, 문드림(Moondream), Gemini Nano 등

### 3.2. 철학

> “모든 사용자가 거대한 서버 없이도 AI 기능을 쓸 수 있게 하자.”
> 

즉, **접근성과 효율성**을 우선시한다.

파운데이션 모델의 ‘크기 경쟁’에서 벗어나

“**서비스 가능한 AI**”로의 패러다임 전환을 상징한다.

---

## 4. 경량화 모델 설계 접근법

### 4.1. Small Language Model(SLM)부터 시작

비전 언어 모델은 “**언어 모델에 눈을 붙인 형태**”다.

따라서 경량화의 시작은 **언어 모델부터 작게 만드는 것**이다

- SLM: Small Language Model
- SLLM: Small Large Language Model (대규모 모델을 경량화한 형태)

이러한 모델은 파라미터를 줄이면서도 **의미 이해 능력**을 유지하도록 설계된다.

---

## 5. 대표 Small VLM 사례

### 5.1. **Small VLM (Hugging Face)**

- Language Model: 1.7B 파라미터
- Vision Backbone: SigLIP
- 구조: Projection + Pooling → LLaVA 유사 구조
- **특징**: GPU 5GB 수준으로 작동 가능
    
    → 모바일, NPU 탑재 노트북 등에서도 구동 가능
    

> “5GB면 스마트폰 NPU에서도 충분히 가능하다.”
> 

---

### 5.2. **Moondream (문드림)**

- 0.5B 파라미터로 더 작음
- 실시간보단 ‘**준실시간(on-edge)**’ 수준
- **400MB 이하 모델 사이즈**, GPU 메모리 < 1GB
- 주요 기능: 이미지 캡셔닝, 비전 QA, 객체 탐지, 시선(Gaze) 인식

→ 실제 비디오챗 등 모바일 응용 가능성이 높음

---

### 5.3. **Gemini Nano (Google Pixel / Samsung)**

- **스마트폰 온디바이스 탑재형 모델**
- 카메라·녹음·대화 요약 등 실시간 기능 내장
- 클라우드 전송 없이 **NPU 기반 로컬 처리**
- 삼성 갤럭시·픽셀폰 등에 내장되어 서비스 중

> “스마트폰에서도 자연어로 ‘스케이트보드 탈 때 찍어줘’라고 말하면,
> 
> 
> AI가 카메라 촬영을 알아서 수행한다.”
> 

이는 AI의 **로컬화(Localized Intelligence)**를 상징한다.

---

## 6. 배포 및 활용 관점

### 6.1. LM Deploy

- 오픈소스 기반 **모델 서빙/압축/배포 프레임워크**
- Small VLM 서포트 내장 → REST API 형태로 배포 가능
- 프론트엔드·백엔드 연계가 쉬워 프로젝트 실용성이 높음

### 6.2. 활용 철학

> “작은 모델일수록 빠르게 배포되고, 더 많은 곳에서 쓸 수 있다.”
> 

이것이 ‘소형 AI’가 갖는 가장 큰 실무적 가치이다.

---

## 7. 경량화의 기술적 도전

- **Trade-off 존재**: 작아질수록 성능 저하 불가피
- **디스틸레이션(Distillation)** 등으로 보정 가능
- **성능 유지율(Defense rate)**이 중요한 지표

결국 “**성능을 덜 잃고 얼마나 작게 만들 수 있느냐**”가 핵심이다.

---

## 8. 한국어 Small VLM의 필요성

### 8.1. 왜 따로 만들어야 하나?

한국어는 영어보다 **토크나이징이 비효율적**이다.

→ 토큰 수가 많고, 계산량·비용이 급증한다

- 영어: 1단어 ≈ 5 토큰
- 한국어: 효율적일 때 8~13 토큰 (2배 이상)
- 토큰이 길면 메모리, 속도, API 과금 모두 증가

> “한글로 물어보는 게 영어보다 비쌉니다.”
> 

따라서 한국어의 형태소 구조에 맞춘 **전용 토크나이저**가 필수다.

---

## 9. 한국형 Small VLM 사례 — Naver HyperCLOVA X

- **3B 파라미터** 모델 (한국어 + 비전 통합)
- **텍스트·이미지 동시 이해 및 생성** 가능
- 성능 비교: GPT-4 수준의 정량지표,
    
    한국어 검정고시 문제에서는 **GPT보다 높은 이해력**
    

**특징 요약**

- 한국어뿐만 아니라 일본어까지 일정 수준 이해
- 이미지 기반 질문, 레시피 생성, 손글씨 차트 분석 등 가능
- Hugging Face에서 직접 다운로드 및 실행 가능

---

## 10. 교수님의 핵심 메시지 요약

| 주제 | 핵심 설명 |
| --- | --- |
| **대형 모델 한계** | 비용·자원 문제로 실사용이 어려움 |
| **경량화 필요성** | 모든 사용자가 접근 가능한 AI 실현 |
| **Small VLM 철학** | “효율·접근성·온디바이스 실행” 중심 |
| **한국어 특수성** | 토크나 복잡성으로 인한 효율 저하 |
| **산업적 의미** | 클라우드 의존형 AI에서 **로컬 AI 시대**로의 전환 |

---

## 11. 정리

> “작은 모델은 단순히 축소판이 아니다.
> 
> 
> **현실에서 돌아가는 AI**로서의 방향성을 제시한다.”
>