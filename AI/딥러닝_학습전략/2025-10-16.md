# 딥러닝 학습 실전: “손맛”으로 불안정성을 다루는 법

## 0. 강의 맥락

> “모델을 잘 짠다고 자동으로 학습이 되는 건 아닙니다. 학습의 불안정성을 다루는 ‘손맛’이 필요해요.”
> 

이 강의는 **수학 공식보다 실전 조정법**에 초점을 둡니다.

초반 학습 불안정, 손실 폭발·정체, 느린 수렴, 과적합에 대응하기 위해 **활성화–초기화–정규화–전처리–학습률·스케줄–옵티마–탐색 순서**로 문제를 구조화합니다.

---

## 1. 우리가 잡아야 할 것: **학습의 불안정성**

- 초반 학습에서 가장 자주 겪는 이슈: **손실 폭발**, **손실 정체**, **과적합**, **느린 수렴**.
- 연산 자원과 시간은 제한되어 있으므로, **무작정 전수(全數) 시도 금지**.
    
    → “**전략**을 세워 빨리 판단하고 다음 단계로 넘어가는 감각”이 핵심.
    

---

## 2. 학습 중 마주치는 주요 증상과 1차 질문

- **손실이 폭발**한다 → 학습률이 너무 큼? 초기화가 부적절? (특히 깊은 네트워크)
- **손실이 전혀 안 줄고 유지** → 초기화 문제? (초기 손실부터 비정상) 혹은 학습률이 너무 작음
- **수렴이 매우 느림** → 학습률·스케줄·모멘텀 설정 미스, 정규화·배치 크기 불균형
- **과적합**(훈련↑/검증↓) → 정규화 부족, 모델 과복잡, 데이터 부족
- **훈·검 둘 다 낮음** → 모델이 약함(용량 부족), 학습 시간/데이터/구조 보강 필요

---

## 3. 집중 컨트롤 항목

### 3.1 활성화 함수 선택 — “**ReLU부터** 시작”

> “ReLU랑 He 초기화는 정말 잘 어울립니다. ReLU 쓰면서 Xavier 쓰면 초반에 gradient가 죽어요.”
> 
- **Sigmoid / Tanh**
    - 장점: 확률 해석(출력 제한), 0 중심 대칭(tanh)
    - 단점: **기울기 소실**, 계산 비쌈(지수 포함)
- **ReLU**: f(x)=max⁡(0,x)f(x)=\max(0,x)f(x)=max(0,x)
    - 장점: **포화 적음 → gradient 살아 있음**, 계산 저렴, 학습 빠름(경험적으로 “수 배” 빠르게 수렴)
    - 단점: 음수 영역 **죽은 뉴런**(bias 쏠림)
- **Leaky ReLU**: 음수 영역 작은 기울기로 보정(죽은 뉴런 완화)
- **ELU**: 음수 영역을 지수로 스무딩(0 근처 평균 유도) ↔ 계산 복잡·하이퍼(α) 필요

**실전 권고**

1. **ReLU로 시작** → 필요 시 Leaky ReLU/ELU로 보정.
2. 분포·gradient 흐름에 ‘한쪽 쏠림’이 보이면 변종 고려.

### 3.2 데이터 전처리(입력 정규화 & 일관성)

- **학습/검증/테스트에 동일한 정규화**를 적용(평균·표준편차 일치).
- 모델별 관행 예
    - AlexNet: **평균 이미지**를 입력에서 차감
    - VGG: **채널별 평균 차감**
    - ResNet: **평균 차감 + 표준편차 나눔**
- 해상도·채널 수를 **모델 입력에 맞춰 고정**(grayscale→3채널 복제 등).

### 3.3 초기화 철학 — “대칭·분산 **유지** vs. ReLU의 **비대칭**”

- **영(0) 초기화 금지**: 대칭 깨지지 않아 학습 정지.
- **작은 랜덤만**으로는 깊은 네트워크에서 **gradient 소실**.
- **Xavier 초기화**: 대칭 활성화(tanh 등) 가정, **입력·출력 분산 매칭** 철학.
- **He 초기화**: ReLU의 **비대칭(음수 사망)**을 고려해 분산 보정.
    
    $\text{He init:}\quad W \sim \mathcal{N}\!\left(0, \sqrt{\frac{2}{n_{\text{in}}}}\right)$
    
- **ResNet 초기화 팁**
    - 잔차 블록의 **첫 번째 Conv는 He**, **두 번째 Conv의 가중치는 0으로** 시작 → 초기에는 skip 경로 위주로 흘러 **안정적 출발**, 이후 점진적 업데이트.

---

## 4. 정규화(Regularization) — “과적합 신호에 즉시 대응”

> “Validation 오차가 올라가면 과적합의 신호입니다.”
> 
- **L2(Weight Decay, Ridge)**: 큰 가중치 억제 → 안정화
- **L1(Lasso)**: 희소성 유도(효과적으로 **모델 단순화** 방향)
- **Elastic Net**: L1+L2 절충
- **Dropout**: 뉴런 무작위 off → 과특화 방지, **앙상블 유사 효과**
    - 주의: **CNN에서 BN와 함께면 드롭아웃 빈도 낮음**, 추론 시 스케일 보정 필요

---

## 5. 학습률(LR)과 스케줄 — “**가장 크리티컬**한 하이퍼파라미터”

> “학습률 틀리면 나머지 다 좋아도 망합니다. 로그 스케일로 탐색하세요.”
> 
- **Step Decay**: 특정 에폭마다 LR 급감(초반 빠르게, 후반 미세하게)
- **Cosine Annealing**: 코사인 곡선으로 **부드럽게 감소**(파인튜닝에서 특히 안정)
- 그 외 Linear decay, Inverse-sqrt, Large-data **constant LR**
- **Adam**처럼 적응형 옵티마도 **별도 LR 스케줄**을 함께 쓰는 경우 많음

---

## 6. Optimizer 설정의 핵심

- **Momentum**: 관성으로 진동 감소·수렴 가속
- **Weight Decay**: L2 규제(과적합 억제)
- **Batch Size / Epoch**: 자원과 안정성의 균형(너무 큰 배치는 일반화 저하 가능)

> “모멘텀은 빠르게, 가중치 감쇠는 안정적으로. 둘을 같이 가져가세요.”
> 

---

## 7. Early Stopping

- 검증 손실이 **개선 없고 악화**되기 시작하면 **정지**.
- “오래 돌리면 언젠가 과적합” → **멈출 타이밍**을 검증 곡선으로 판단.

---

## 8. 하이퍼파라미터 탐색 — **Grid보다 Random**

> “모든 조합을 끝까지 돌리는 건 자원 낭비입니다.”
> 
- **Random Search**가 중요한 축을 넓게, 빠르게 커버.
- 먼저 **핵심 축(학습률·가중치감쇠·모멘텀·스케줄)**을 정해 압축 탐색.

---

## 9. 교수님의 **6단계 체크리스트** (실전 루틴)

### ✅ 체크리스트 1 — **데이터 입력**부터 의심

- **초기 손실값**이 **log⁡(클래스 수)\log(\text{클래스 수})log(클래스 수)** 근처면 정상.
- 크게 벗어나면 **데이터 로딩/정규화/레이블** 문제.

### ✅ 체크리스트 2 — **학습률·초기화 점검 (의도적 과적합)**

- 아주 **작은 샘플**로 **정규화 없이 100% 정확도**까지 외우게 해본다.
    - 실패 시: 학습률/초기화/활성화/Optimizer/코드 버그 의심
    - 손실 폭발: **LR ↓**, 초기화 변경, 구조(깊이) 이슈 점검

### ✅ 체크리스트 3 — **최적 LR 찾기 (전 데이터, 극초기)**

- **전 학습데이터 사용** + **아주 작은 iteration(≈100)**으로
- **손실이 실제로 줄어드는 LR 구간**을 **로그 스케일**로 탐색

### ✅ 체크리스트 4 — **초기 LR × 스케줄 조합 선별(1–5 epoch)**

- 후보 **LR ×(Step/Cosine/Flat …)** 조합을 **1–5 epoch만** 빠르게 비교
- **검증 성능 기준**으로 **빠른 손실 감소** 조합만 남긴다

### ✅ 체크리스트 5 — **스케줄 미적용으로 순수 조합력 평가(10–20 epoch)**

- 스케줄은 성능↑ but 속도↓ → **일단 빼고** 조합의 순수 성능으로 선별

### ✅ 체크리스트 6 — **손실 곡선 해석으로 원인 추적**

- 초반 **손실이 그대로 유지** → **초기화 문제** (값 교체)
- 손실이 **떨어지다 평평(plateau)** → **LR 스케줄** 시점
- 스케줄 후에도 **정체** → **너무 빨리 줄였다**(스케줄 타이밍 뒤로)
- **학습↑/검증↓** → **과적합**(정규화·증강·모델 약화)
- **학습↓/검증↓**(둘 다 낮음) → **모델 약함**(더 큰 모델·학습 더 오래)

---

## 10. 손실 곡선 **도감(診斷)**

- **폭발**(상승): LR↓, 초기화 교체(깊은 층일수록 He/Tensor-scale 점검)
- **초기 수평 유지**: 초기화 변경, 활성화–초기화 궁합 확인(**ReLU↔He**)
- **중후반 plateau**: Step/Cosine 등 **스케줄 적용** 시점 조정
- **검증만 악화**: 정규화(Weight Decay/Dropout/BN), 데이터 증강
- **둘 다 낮음**: 모델 용량·표현력 확대, 에폭·데이터 증가

---

## 11. 강의 핵심 메시지(정리)

> “좋은 아키텍처만으로는 0점일 수 있습니다. 학습을 끌고 가는 전략이 더 중요할 때가 많아요.”
> 
> 
> “**ReLU + He 초기화**로 시작하고, **학습률을 로그 스케일로** 찾으세요.
> 
> **Validation 오차가 오르면 과적합 신호**, 정규화와 증강으로 대응하세요.”
> 
> “항상 **데이터 → 초기화 → 학습률 → 구조** 순서로 원인을 좁혀가면, 끝내 답을 찾습니다.”
> 

---

### 부록: 자주 쓰는 수식(초기화 예시)

$\text{Xavier (tanh 등 대칭 활성화)}:\quad W \sim \mathcal{N}\!\left(0, \sqrt{\frac{1}{n_{\text{in}}}}\right)\ \text{또는}\ \sqrt{\frac{2}{n_{\text{in}}+n_{\text{out}}}}$

$\text{He (ReLU 계열)}:\quad W \sim \mathcal{N}\!\left(0, \sqrt{\frac{2}{n_{\text{in}}}}\right)$

$\text{ResNet block tip}:\ \text{Conv1: He init,\ Conv2: }W=0\ \ (\text{초기에는 skip 경로 위주로 안정 출발})$

---

# 딥러닝 학습의 핵심 원리와 실전 점검 가이드(중복있을 수 있음)

---

## 1. 학습 과정의 본질 ― “손실을 얼마나, 어떻게 줄일 것인가”

> “딥러닝의 목표는 손실(loss)을 줄이는 것입니다.
> 
> 
> 그런데 단순히 손실이 줄었다고 끝이 아닙니다.
> 
> **손실이 ‘어떻게’ 줄어드는가**를 봐야 합니다.”
> 
- 손실이 안정적으로 감소하지 않으면,
    
    데이터 로딩·초기화·학습률·정규화 등 중 하나가 문제일 가능성이 크다.
    
- 따라서 학습 초반부터 **손실곡선의 형태**를 세밀히 관찰해야 한다.

---

## 2. 초기화와 활성화 함수 ― “ReLU엔 He 초기화가 정답이다”

> “ReLU는 양수만 통과시키기 때문에 분산이 한쪽으로 쏠립니다.
> 
> 
> 그래서 He 초기화가 정말 잘 어울립니다. 거의 ‘궁합’이에요.”
> 

```latex
W \sim \mathcal{N}(0, \sqrt{2/n})

```

- Sigmoid → **Xavier 초기화**
- ReLU → **He 초기화**

> “활성화 함수와 초기화는 따로 보면 안 됩니다.
> 
> 
> 항상 **세트로 생각해야** 해요.”
> 

**핵심 포인트**

- ReLU와 He 초기화는 폭발하지 않으면서 빠르게 학습을 시작하게 함
- 초기 손실이 비정상적으로 크거나 작다면 초기화부터 의심

---

## 3. 학습률 (Learning Rate) ― “가장 크리티컬한 하이퍼파라미터”

> “학습률이 잘못되면, 나머지가 다 좋아도 학습은 안 됩니다.
> 
> 
> 손실이 폭발하거나, 줄지도 않아요.”
> 
- **너무 크면** → 손실 폭발(Exploding Loss)
- **너무 작으면** → 학습 정체(Underfitting)

> “학습률은 로그 스케일로 탐색하세요.
> 
> 
> 손실이 실제로 줄어드는 구간을 눈으로 확인하는 게 중요합니다.”
> 

### [추가설명]

- Cosine Annealing / Step Decay 등의 스케줄링을 통해 부드럽게 조정 가능
- 손실 폭발 시 → 학습률 감소
- 손실 정체 시 → 학습률 증가

---

## 4. 과적합(Overfitting)의 신호 ― “Validation 오차가 오르면 외우고 있는 겁니다”

> “Validation 오차가 올라가면 과적합의 신호입니다.
> 
> 
> 이건 ‘모델이 외우기 시작했다’는 뜻이에요.”
> 

### [대응 전략]

1. **정규화(Regularization)**
    - Dropout, Weight Decay, Batch Normalization
2. **가중치 크기 제한**
    - 너무 큰 가중치는 불안정한 학습을 초래
3. **데이터 증강(Augmentation)**
    - 다양한 패턴을 학습해 일반화 성능 강화

> “과적합은 ‘모델이 너무 강한데 데이터가 약한 상황’이에요.
> 
> 
> 그러면 모델을 약하게 하거나 데이터를 늘려야 합니다.”
> 

---

## 5. 학습률 스케줄링 ― “계단형보다 코사인이 부드럽다”

> “학습률을 일정하게 두는 건 비효율적이에요.
> 
> 
> 계단형(Step)으로 줄이거나,
> 
> 요즘은 **코사인 방식(Cosine Annealing)** 으로 부드럽게 줄이는 게 좋아요.”
> 
- Step Decay: 일정 epoch마다 감소
- Cosine Annealing: 코사인 형태로 부드럽게 감소

> “코사인 방식은 학습 후반 안정화를 유도하고,
> 
> 
> 특히 **Fine-tuning** 단계에서 효과적입니다.”
> 

---

## 6. 최적화 도구(Optimizer)의 핵심 설정

> “Optimizer는 방향과 속도를 정해주는 엔진이에요.
> 
> 
> 그래서 학습률, 모멘텀, 가중치 감쇠를 함께 봐야 합니다.”
> 
- **Momentum (모멘텀 계수)**
    
    → 관성 효과로 진동 감소, 빠른 수렴
    
- **Weight Decay (가중치 감쇠)**
    
    → 과적합 방지용 L2 규제
    
- **Batch Size / Epoch 수 조정**
    
    → 자원 효율과 안정성의 균형 확보
    

> “모멘텀은 빠르게, 감쇠는 안정적으로.
> 
> 
> 둘을 함께 써야 합니다.”
> 

---

## 7. 교수님의 실전 체크리스트 (1~6단계)

### ✅ 체크리스트 1 — 데이터 입력 문제

> “초기 손실값부터 보세요.
> 
> 
> log(1000) 근처면 정상이에요.
> 
> 그보다 크거나 작으면 데이터 로딩 문제일 확률이 높습니다.”
> 

---

### ✅ 체크리스트 2 — 학습률과 초기화 점검

> “정규화를 다 끄고, 고의로 과적합시켜보세요.
> 
> 
> 100% 정확도까지 가야 정상이에요.”
> 
- 그래도 안 된다면?
    
    → 학습률, 초기화, 활성화 함수, Optimizer, 코드버그 점검
    
- 손실이 폭발한다면?
    
    → 학습률 감소, 초기화 변경
    
- 그래도 손실이 줄지 않으면?
    
    → 모델 구조 이상 or 너무 깊은 네트워크 문제
    

---

### ✅ 체크리스트 3 — 학습률 탐색

> “작은 데이터로 빠르게 실험하세요.
> 
> 
> 손실이 실제로 줄어드는 구간부터 시작하는 게 중요합니다.”
> 
- 로그 스케일로 학습률 탐색
- 안정적 수렴 구간 선택

---

### ✅ 체크리스트 4 — 좋은 조합 탐색

> “좋은 학습률 + 초기화 조합을 찾아서
> 
> 
> 1~5 epoch만 돌려보세요.
> 
> 빨리 줄어드는 조합이 최적입니다.”
> 
- 전체 학습까지 가지 말고 **초기 손실 감소 패턴**으로 판단
- 검증 세트 기준으로 조합 선택

---

### ✅ 체크리스트 5 — 학습률 변경 전 평가

> “좋은 조합을 찾았으면, 학습률 조정 없이 10~20 epoch 돌려보세요.
> 
> 
> 순수한 조합의 힘을 먼저 봐야 합니다.”
> 
- 이후 필요 시 Cosine/Step으로 스케줄링 추가

---

### ✅ 체크리스트 6 — 손실 곡선 분석

> “손실이 초반에 그대로면 초기화 문제입니다.
> 
> 
> 손실이 떨어지다 멈추면 학습률을 의심하세요.”
> 
- 초기 손실 유지 → 초기화 불량
- 손실 정체 → 학습률 불균형
- 학습률 조정에도 변화 없음 → 너무 빨리 조정했거나 구조 문제

**과적합 시**

- 학습 ↑ 검증 ↓ → 정규화 도입
- 학습·검증 모두 낮음 → 모델 약함 → 더 큰 모델 or 더 오래 학습

---

## 8. 교수님의 실전 철학 ― “학습은 원인 추적이다”

> “학습이 잘 안 되면, 무작정 다시 돌리지 말고
> 
> 
> **데이터 → 초기화 → 학습률 → 구조 순서대로 점검하세요.**
> 
> 딥러닝은 모델이 아니라, 문제를 찾아가는 과정입니다.”
> 

---

## 💡 최종 정리 요약표

| 범주 | 핵심 포인트 | 해결 전략 |
| --- | --- | --- |
| 데이터 | 입력 확인 | 초기 손실값 점검 |
| 초기화 | ReLU + He | 함수·초기화 세트로 구성 |
| 학습률 | 가장 중요한 파라미터 | 로그 스케일 탐색 |
| 과적합 | 검증 오차 증가 | 정규화, Dropout, Weight Decay |
| 최적화 | 모멘텀 + 감쇠 병용 | 안정적 수렴 |
| 학습곡선 | 초기 손실 유지 시 문제 | 손실 형태별 원인 구분 |
| 스케줄링 | Cosine Annealing | 후반 안정화 |