# 언어 이해와 순차적 데이터 처리

## 1. 언어 이해의 단계

### 단어 수준의 이해

- 인간이 언어를 이해하는 과정은 **단어 → 문장 → 의미 구조**로 점차 확장된다.
- **단어 이해의 출발점은 심볼(Symbol)** 이며, 컴퓨터는 심볼을 직접 이해하지 못하므로 **숫자 표현으로 변환**해야 한다.
- 가장 단순한 변환 방식은 **원핫 인코딩(One-Hot Encoding)**.

### 원핫 인코딩의 특징

- 단어 집합(Vocabulary)의 크기만큼의 차원을 가진다.
- 각 단어는 하나의 위치만 1, 나머지는 모두 0으로 표현된다.
- 예: `["호텔", "컨퍼런스", "워크"]` 중 “호텔”은 `[1, 0, 0]`.

[추가설명]

실제 언어 모델에서는 단어 집합 크기가 5만~50만 단어에 달하므로, 벡터 차원도 매우 커진다.

### 원핫 인코딩의 한계

1. **의미적 유사성 결여:**
    - “핸드폰”과 “스마트폰”은 의미상 유사하지만, 원핫 벡터에서는 완전히 다른 위치의 1로 표현되어 **내적 결과가 0**이 된다.
2. **차원의 저주(Curse of Dimensionality):**
    - 차원이 커질수록 연산량과 메모리 사용량이 급증한다.

---

## 2. 단어의 의미적 표현: Word Embedding

### 개념

- 단어를 **의미 정보를 보존한 연속적 벡터**로 표현하는 방법.
- 원핫 인코딩의 단점을 해결하기 위해 제안된 개념.
- **주변 문맥(Context)** 정보를 이용해 단어 의미를 학습한다.

[추가설명]

“은행”이라는 단어는 “대출”, “돈”, “규제” 같은 문맥에서 등장할 가능성이 높다.

모델은 이런 **문맥 기반 통계적 관계**를 이용해 벡터를 학습한다.

### 대표 기법: Word2Vec

- Word Embedding을 실현하는 대표적 알고리즘.
- 두 가지 구조:
    1. **CBOW (Continuous Bag of Words):** 주변 단어로 중심 단어를 예측.
    2. **Skip-Gram:** 중심 단어로 주변 단어를 예측.
- 두 구조의 핵심은 **“주변 단어가 함께 등장할 확률을 학습”**하는 것.

[추가설명]

Skip-Gram은 단어마다 하나씩 예측하므로 데이터가 많고 희귀 단어 학습에도 유리하다.

### 학습 과정

1. 중심 단어를 입력(인코더).
2. 50만 차원의 원핫 벡터를 **100~300차원의 임베딩 벡터**로 압축.
3. 출력(디코더)은 다시 50만 차원 확률 분포로 복원.
4. 실제 정답 단어와의 차이를 **loss로 계산**, 반복 학습.

### 결과

- 모델이 학습한 100차원 벡터 공간에서는 유사한 단어들이 **가까운 위치**에 존재.
- 의미 기반 유사도 계산(내적 또는 코사인 유사도)이 가능해진다.

---

## 3. 순차적 데이터 처리의 필요성

### 단어 독립의 한계

- Word2Vec은 단어 단위의 표현을 학습하지만, **단어 간 순서나 문맥 구조**를 반영하지는 못한다.
- 문장은 단어의 순서에 따라 의미가 완전히 달라질 수 있다.
    
    예: “나는 너를 사랑해” ↔ “너를 나는 사랑해”
    

### 순차 데이터(Sequence)의 특징

- 길이가 가변적이다. (입력 문장마다 단어 수가 다름)
- 텍스트 외에도 **음성, 비디오, 시계열** 등 다양한 데이터가 이에 해당한다.
- 이를 처리하기 위해 **RNN(Recurrent Neural Network)** 구조가 등장했다.

---

## 4. RNN의 기본 구조와 동작

### 핵심 아이디어

- 이전 시점의 정보를 **히든 스테이트(hidden state)** 로 저장하고, 현재 입력과 결합하여 다음 출력을 생성.
- **시계열적 의존성**을 학습할 수 있다.

### 구조적 특징

- 같은 가중치(W)를 **모든 타임스텝에 반복 적용**한다.
- 입력 차원: 단어 벡터(예: 5만차원 원핫 or 100차원 임베딩)
    
    히든 차원: 모델이 설정한 임의의 크기(예: 100차원)
    
- 반복 계산:
    
    ht=f(Wxhxt+Whhht−1)h_t = f(W_{xh}x_t + W_{hh}h_{t-1})ht=f(Wxhxt+Whhht−1)
    

[추가설명]

RNN은 파라미터 공유 덕분에 문장의 길이에 관계없이 동일한 구조로 처리 가능하다.

### 활용 예시

- **입력:** 영화 리뷰 문장 (가변 길이)
- **출력:** 긍정/부정 확률 (2차원)
- **마지막 단계:** Softmax를 통해 분류 결과 계산.

---

## 5. RNN의 한계와 LSTM의 등장

### RNN의 한계

- 긴 문장에서 초반 정보가 점차 소실되는 **Vanishing Gradient** 문제가 발생.
- 문장 앞의 단어가 뒤로 갈수록 영향력이 급격히 줄어든다.

[추가설명]

100페이지 책을 읽고 마지막에 내용을 요약하려 하면 앞부분이 잘 기억나지 않는 것과 같은 현상이다.

---

## 6. LSTM (Long Short-Term Memory)

### 개념

- RNN의 단기 기억 한계를 해결하기 위해 고안된 구조.
- **두 개의 상태**를 유지한다:
    1. **Hidden State (단기 기억)**
    2. **Cell State (장기 기억)** — 클립보드처럼 저장소 역할.

### 게이트 구조

LSTM은 세 가지 게이트를 통해 정보를 제어한다.

1. **Forget Gate (망각 게이트)**
    - 이전 정보 중 얼마나 버릴지를 결정.
2. **Input Gate (입력 게이트)**
    - 새로운 정보 중 얼마나 저장할지를 결정.
3. **Output Gate (출력 게이트)**
    - 저장된 정보 중 현재 출력에 얼마나 반영할지를 결정.

### 작동 예시

- Forget Gate = 0, Input Gate = 1 → **이전 기억을 버리고 새로운 정보에 집중.**
- Forget Gate = 1, Input Gate = 0 → **기존 정보를 유지.**

[추가설명]

교수님은 “수업 중 첫사랑 얘기” 비유로 설명.

RNN은 불필요한 이야기까지 다 저장하지만, LSTM은 게이트를 통해 **필요한 정보만 남기고 불필요한 것은 버림.**

### 연산 방식

- 각 게이트는 시그모이드(0~1 확률)로 작동.
- 게이트 간 곱 연산은 **element-wise multiplication**으로 수행되어, 벡터 차원별로 선택적 반영이 가능하다.

---

## 7. 정리 및 인사이트

- **Word Embedding**: 단어를 의미 벡터로 표현.
- **RNN**: 단어 순서와 문맥을 반영하는 기본 순차 모델.
- **LSTM**: 장기 문맥 기억을 가능하게 한 RNN의 개선형.

[추가설명]

이후 등장하는 **트랜스포머(Transformer)** 는 LSTM의 한계를 넘어, 병렬 계산과 긴 문맥 학습을 가능하게 만든 모델로 발전한다.

---

## 핵심 요약

- 원핫 인코딩 → Word Embedding → RNN → LSTM 순으로 언어 이해가 발전.
- Word2Vec은 문맥 기반의 단어 의미를 학습하는 초기 혁신적 기법.
- RNN은 순서를 반영하지만, 긴 문장에서 정보 손실 발생.
- LSTM은 게이트 구조로 중요 정보만 유지하여 장기 의존성 문제를 해결.
- 현대 언어 모델(예: GPT)은 LSTM 이후의 **Transformer 구조** 위에서 동작한다.